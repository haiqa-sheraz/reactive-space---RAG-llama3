{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10056807,"sourceType":"datasetVersion","datasetId":6197013}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q datasets sentence-transformers faiss-cpu accelerate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install PyPDF2 datasets\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install bitsandbytes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PyPDF2 import PdfReader\nfrom datasets import Dataset\n\n# Step 1: Define a function to extract text from a PDF\ndef extract_text_from_pdf(pdf_path):\n    reader = PdfReader(pdf_path)\n    text = \"\"\n    for page in reader.pages:\n        text += page.extract_text()\n    return text\n\n# Step 2: Extract text from the PDF\npdf_path = \"/kaggle/input/technical-test-new/Technical Test.pdf\"  # Replace with your PDF file path\nif os.path.exists(pdf_path):\n    extracted_text = extract_text_from_pdf(pdf_path)\nelse:\n    raise FileNotFoundError(f\"PDF file not found at path: {pdf_path}\")\n\n# Step 3: Create a dataset with one column for text\ndata = {\"text\": [extracted_text]}  # A single row with the extracted text\ndataset = Dataset.from_dict(data)\n\n# Step 4: Save the dataset (Optional)\ndataset.save_to_disk(\"pdf_text_dataset\")  # Saves dataset locally\n\n# Step 5: Load and view the dataset (Optional)\nloaded_dataset = Dataset.load_from_disk(\"pdf_text_dataset\")\nprint(loaded_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:11:02.904976Z","iopub.execute_input":"2024-12-02T08:11:02.905737Z","iopub.status.idle":"2024-12-02T08:11:04.314980Z","shell.execute_reply.started":"2024-12-02T08:11:02.905687Z","shell.execute_reply":"2024-12-02T08:11:04.314162Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10561aa466943f89603c077936e0b3e"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['text'],\n    num_rows: 1\n})\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nST = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:11:04.876696Z","iopub.execute_input":"2024-12-02T08:11:04.877286Z","iopub.status.idle":"2024-12-02T08:11:39.616040Z","shell.execute_reply.started":"2024-12-02T08:11:04.877248Z","shell.execute_reply":"2024-12-02T08:11:39.614970Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701cafb2eeb14179bb2d744a3ee791d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/266 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d631b77256124303ad24689025f99818"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/114k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ffcf6003db45dd96121de6e56c79d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcbf26c316c843bb9da5ff1d56e6b0f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8881dad2db9842279cf2828d96b54ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f74b7fc1cf4cf3b655e0ac56d10327"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0563d3b07cb94a248d7207a866209284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499fa1b3623c4711bc7fc0132ab22760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82a640d1218421f84afc0582ee924bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220e3f2984b347ed96732bccd0f2b1d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d38333fecaf41d29f98c4051775b490"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"print(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:11:45.013220Z","iopub.execute_input":"2024-12-02T08:11:45.013506Z","iopub.status.idle":"2024-12-02T08:11:45.018216Z","shell.execute_reply.started":"2024-12-02T08:11:45.013479Z","shell.execute_reply":"2024-12-02T08:11:45.017414Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text'],\n    num_rows: 1\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def embed(batch):\n    \"\"\"\n    adds a column to the dataset called 'embeddings'\n    \"\"\"\n    # or you can combine multiple columns here\n    # For example the title and the text\n    information = batch[\"text\"]\n    return {\"embeddings\" : ST.encode(information)}\n\ndataset = dataset.map(embed,batched=True,batch_size=16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:11:59.851634Z","iopub.execute_input":"2024-12-02T08:11:59.851994Z","iopub.status.idle":"2024-12-02T08:12:12.369383Z","shell.execute_reply.started":"2024-12-02T08:11:59.851959Z","shell.execute_reply":"2024-12-02T08:12:12.368376Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c70874213b04a61aebdb2cb8a275a57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed94ffadcde48ab99f3ed58c8d40047"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"\ndata = dataset.add_faiss_index(\"embeddings\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:12:16.412950Z","iopub.execute_input":"2024-12-02T08:12:16.413363Z","iopub.status.idle":"2024-12-02T08:12:16.458193Z","shell.execute_reply.started":"2024-12-02T08:12:16.413332Z","shell.execute_reply":"2024-12-02T08:12:16.457323Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc82f19ad78438893a573a1a17323f8"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"print(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:12:20.288331Z","iopub.execute_input":"2024-12-02T08:12:20.289183Z","iopub.status.idle":"2024-12-02T08:12:20.293513Z","shell.execute_reply.started":"2024-12-02T08:12:20.289148Z","shell.execute_reply":"2024-12-02T08:12:20.292544Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text', 'embeddings'],\n    num_rows: 1\n})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def search(query: str, k: int = 1 ):\n    \"\"\"a function that embeds a new query and returns the most probable results\"\"\"\n    embedded_query = ST.encode(query) # embed new query\n    scores, retrieved_examples = data.get_nearest_examples( # retrieve results\n        \"embeddings\", embedded_query, # compare our new embedded query with the dataset embeddings\n        k=k # get only top k results\n    )\n    return scores, retrieved_examples\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:12:24.357759Z","iopub.execute_input":"2024-12-02T08:12:24.358150Z","iopub.status.idle":"2024-12-02T08:12:24.363225Z","shell.execute_reply.started":"2024-12-02T08:12:24.358115Z","shell.execute_reply":"2024-12-02T08:12:24.362270Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token = \"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:12:28.024618Z","iopub.execute_input":"2024-12-02T08:12:28.025259Z","iopub.status.idle":"2024-12-02T08:12:28.073620Z","shell.execute_reply.started":"2024-12-02T08:12:28.025222Z","shell.execute_reply":"2024-12-02T08:12:28.072720Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# use quantization to lower GPU usage\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    quantization_config=bnb_config\n)\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:12:35.626460Z","iopub.execute_input":"2024-12-02T08:12:35.627092Z","iopub.status.idle":"2024-12-02T08:19:43.448037Z","shell.execute_reply.started":"2024-12-02T08:12:35.627039Z","shell.execute_reply":"2024-12-02T08:19:43.447291Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9ab74753dfe4f34acb45f07ceaaea6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b601a4809f644b85926a885460ac44bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d674c6f911a4b17ae197ea12b0fadba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61fd3636fb9a43ac81e3099f3eacb29c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4127c28e39a34960a77b142f523e1e09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90cbd4dbb4964526b165bc348737ca1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e07e61870b6b4d919a08fe54e38d39c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e2db10f0194149a62b406a802f7f71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3301f636fa344c2c9b4d9c2669ed8b36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"018e2e1327a24a09b2cedc07ccad8d68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12214d95f3ef4599a33f3deded79300c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce9080665b674a048863517f3c922959"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"SYS_PROMPT = \"\"\"You are an assistant for answering questions.\nYou are given a document and a question. Provide a conversational answer.\nIf you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:20:50.767338Z","iopub.execute_input":"2024-12-02T08:20:50.767716Z","iopub.status.idle":"2024-12-02T08:20:50.772180Z","shell.execute_reply.started":"2024-12-02T08:20:50.767685Z","shell.execute_reply":"2024-12-02T08:20:50.771118Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def format_prompt(prompt,retrieved_documents,k):\n  \"\"\"using the retrieved documents we will prompt the model to generate our responses\"\"\"\n  PROMPT = f\"Question:{prompt}\\nContext:\"\n  for idx in range(k) :\n    PROMPT+= f\"{retrieved_documents['text'][idx]}\\n\"\n  return PROMPT\n\ndef generate(formatted_prompt):\n  formatted_prompt = formatted_prompt[:2000] # to avoid GPU OOM\n  messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":formatted_prompt}]\n  # tell the model to generate\n  input_ids = tokenizer.apply_chat_template(\n      messages,\n      add_generation_prompt=True,\n      return_tensors=\"pt\"\n  ).to(model.device)\n  outputs = model.generate(\n      input_ids,\n      max_new_tokens=1024,\n      eos_token_id=terminators,\n      do_sample=True,\n      temperature=0.6,\n      top_p=0.9,\n  )\n  response = outputs[0][input_ids.shape[-1]:]\n  return tokenizer.decode(response, skip_special_tokens=True)\n\ndef rag_chatbot(prompt:str,k:int=2):\n  scores , retrieved_documents = search(prompt, k)\n  formatted_prompt = format_prompt(prompt,retrieved_documents,k)\n  return generate(formatted_prompt)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:20:52.809406Z","iopub.execute_input":"2024-12-02T08:20:52.810166Z","iopub.status.idle":"2024-12-02T08:20:52.817200Z","shell.execute_reply.started":"2024-12-02T08:20:52.810128Z","shell.execute_reply":"2024-12-02T08:20:52.816134Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"rag_chatbot(\"what's the required technical stack?\", k = 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:20:56.121967Z","iopub.execute_input":"2024-12-02T08:20:56.122733Z","iopub.status.idle":"2024-12-02T08:21:13.270843Z","shell.execute_reply.started":"2024-12-02T08:20:56.122696Z","shell.execute_reply":"2024-12-02T08:21:13.269856Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b9590006004fbba31f98b57bf9f254"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"\"Based on the provided context and requirements, the required technical stack for creating an intelligent chatbot agent that can engage with users about company information and handle email communications includes:\\n\\n* Backend: Node.js or Python (candidate's choice)\\n* Database: Any vector database of your choice\\n* Required Libraries:\\n\\t+ LangChain or equivalent for RAG implementation\\n\\t+ WebSocket for real-time communication\\n\\t+ Any UI component library of your choice\\n\\nThe LangChain library is specifically mentioned for implementing RAG (Retrieval Augmented Generation) features. Additionally, WebSocket is required for real-time communication features.\\n\\nPlease note that the choice of UI component library is left to the candidate, and any suitable library can be used.\""},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"rag_chatbot(\"tell me about the conversation flow?\", k = 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:21:23.301679Z","iopub.execute_input":"2024-12-02T08:21:23.302041Z","iopub.status.idle":"2024-12-02T08:21:57.652090Z","shell.execute_reply.started":"2024-12-02T08:21:23.302008Z","shell.execute_reply":"2024-12-02T08:21:57.650987Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7475b3f634a4bf7985e7afa2cc76e5d"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"Based on the provided context, I'll walk you through the conversation flow of the intelligent chatbot agent.\\n\\nThe conversation flow is designed to engage users about company information and handle email communications. Here's an overview of how it works:\\n\\n1. **Company Overview**: The chatbot starts by presenting a brief company overview (maximum 2 sentences) to introduce itself.\\n2. **Asking for Interest**: The chatbot asks the user if they want to know more about the company. This is done to gauge the user's interest and determine the next course of action.\\n3. **Retrieving Information**: If the user agrees to know more, the chatbot retrieves relevant information from the knowledge base. This is where the Retrieval Augmented Generation (RAG) implementation comes into play, using LangChain or equivalent to generate contextual responses based on the user's input.\\n4. **Offering Company Profile**: The chatbot offers to send the company profile via email. This is where the user's consent is obtained, and their email address is collected and confirmed.\\n5. **Email Confirmation**: After the user agrees to receive the company profile, the chatbot sends a confirmation message and dispatches the email.\\n6. **Real-time Chat Interface**: Throughout the conversation, the chatbot maintains a real-time chat interface using WebSocket connections. This allows for seamless communication and updates the message history in real-time.\\n\\nThe conversation flow is designed to be flexible, allowing the chatbot to adapt to user input and provide accurate information. The RAG implementation ensures that the chatbot can generate contextual responses based on the user's queries, making the conversation more engaging and informative.\\n\\nI hope this helps! Let me know if you have any further questions.\""},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"rag_chatbot(\"discuss submission requirements\", k = 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:21:57.669313Z","iopub.execute_input":"2024-12-02T08:21:57.669525Z","iopub.status.idle":"2024-12-02T08:22:33.279977Z","shell.execute_reply.started":"2024-12-02T08:21:57.669502Z","shell.execute_reply":"2024-12-02T08:22:33.279072Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"162c681b6d0349a79b21eb67e518c438"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"\"I'd be happy to discuss the submission requirements for creating an intelligent chatbot agent.\\n\\nTo submit your project, you will need to provide the following:\\n\\n1. Source code via a GitHub repository: This should include the implementation of the chatbot agent's conversation flow, RAG (Retrieval Augmented Generation) functionality, and real-time communication features.\\n2. README with:\\n\\t* Setup instructions: This should include step-by-step instructions on how to set up and run your chatbot agent.\\n\\t* Architecture overview: This should provide a high-level overview of the chatbot agent's architecture, including the technical stack, database, and required libraries.\\n\\t* Key design decisions: This should explain the design choices you made for the chatbot agent, including why you chose certain technologies and approaches.\\n\\t* Known limitations: This should list any limitations or issues you encountered during development.\\n\\t* Future improvements: This should outline potential future improvements or enhancements you would like to make to the chatbot agent.\\n3. Brief documentation of RAG implementation: This should provide a detailed explanation of how you implemented the RAG functionality, including how you created and managed vector embeddings, implemented relevant search functionality, and used search results to generate contextual responses.\\n\\nThe evaluation criteria for this project are:\\n\\n* Technical Implementation (60%): This will assess the correctness of the RAG implementation, proper handling of WebSocket connections, clean and efficient code structure, error handling, and code documentation.\\n* Conversation Flow (40%): This will evaluate the proper implementation of conversation states, accurate information retrieval, email handling workflow, and response appropriateness.\\n\\nI hope this helps! If you have any further questions or need clarification on any of the requirements, feel free to ask.\""},"metadata":{}}],"execution_count":14}]}